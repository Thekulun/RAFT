# RAFT
## ABSTRACT
Pre-trained code language models (e.g. CodeBERT and CodeT5) have demonstrated commendable performance in code generation tasks, such as code summarization and code generatation. Fine-tuning through maximum likelihood estimation (MLE) is a typical approach to adapt pre-trained code language models to downstream tasks. Due to its data-driven nature, fine-tuning pre-trained code language models often requires large and high-quality training data to achieve significant performance. Previous studies have employed data filtering to allievate the data quality issue, and data augmentation to allievate the data scarcity issue. However, these data filtering and data augmentation approaches proposed for code language models are limited by heuristic rules or involve relatively complex processes such as back translation. In light of these issues, we propose RAFT-DF, a data filtering approach to fine-tune pre-trained code language models based on Reward Ranked FineTuning (RAFT), which was originally designed for aligning generative models with human preferences. Additionally, we present a data augmentation approach, RAFT-SI, through integrating RAFT and Self-Improved. The fundamental idea underlying both approaches entails scoring the outputs generated by initially fine-tuned models to identifying high-quality ones. The filtered dataset is used to iteratively fine-tune the last round one. We instantiate the RAFT-DF and RAFT-SI approaches with various pre-trained code language models including CodeBERT, CodeT5, and UniXCoder. The experimental results on CodeSearchNet and CodeXGLUE benchmark demonstrate that both RAFT-DF and RAFT-SI improve the performance of these code language models on code summarization and code generation tasks. Furthermore, we investigate the effectiveness of the approaches on the interesting cross-lingual scenarios. Additionally, we conduct an abalation study on the impact of different reward functions and hyperparameters on both approaches.
